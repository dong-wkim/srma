{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e16ca440-96c8-43ef-9d58-569ed27c7926",
   "metadata": {},
   "source": [
    "# Systematic Review and Meta-Analysis Documentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f07f13-d605-4b61-aa0b-96048da7515d",
   "metadata": {},
   "source": [
    "## Protocol"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac67675-f008-49ca-ac7f-a250e1ff7aa7",
   "metadata": {},
   "source": [
    "| | | | |\n",
    "|:--|:--|--|--|\n",
    "| Review title                                        | Peroneus longus tendon with or without peroneus brevis tenodesis for primary anterior cruciate ligament reconstruction surgery: A systematic review and meta-analysis.                                                                                                                                                                                                                                                                                                                                                   |             |               |\n",
    "| Condition or domain being studied                   | Knee and ankle outcome measures after primary anterior cruciate ligament reconstruction surgery using peroneus longus tendon autograft.                                                                                                                                                                                                                                                                                                                                                                                  |             |               |\n",
    "| Rationale for the review                            | A systematic review and meta-analysis of studies reporting outcomes using the peroneus longus tendon for ACLR –- whether single-arm syntheses or comparison between two methods of harvesting is not available in current literature. Results of this study will add significant value to the knowledge surrounding available graft options for ACLR surgery.                                                                                                                                                            |             |               |\n",
    "| Original language title                             | English                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |             |               |\n",
    "| Review objectives                                   | Are the knee function outcomes of peroneus longus tendon autograft comparable to those of other graft options? Do the ankle function outcomes outweigh the donor-site morbidity and adverse events that arise from other graft harvesting locations? How does peroneus longus tendon harvesting with and without peroneus brevis tenodesis affect knee and ankle outcomes?                                                                                                                                               |             |               |\n",
    "| Keywords                                            | anterior cruciate ligament reconstruction, ACLR, peroneus longus, PLT, fibularis longus, FLT, autograft                                                                                                                                                                                                                                                                                                                                                                                                                  |             |               |\n",
    "| Searches                                            | PubMed/MEDLINE, Embase, Web of Science, Scopus, Cochrane Library                                                                                                                                                                                                                                                                                                                                                                                                                                                         |             |               |\n",
    "| Study design                                        | Systematic review and meta-analysis of randomized clinical trials, prospective cohort studies, retrospective cohort studies, case-control studies, and case series. Level of evidence (IV)                                                                                                                                                                                                                                                                                                                               |             |               |\n",
    "| Population                                          | Skeletally-mature adult patients defined as ≥ 18 years old.                                                                                                                                                                                                                                                                                                                                                                                                                                                              |             |               |\n",
    "| Intervention(s) or exposure(s)                      | Peroneus longus tendon (PLT) graft harvest with or without distal attachment to peroneus brevis tendon (PLBT).                                                                                                                                                                                                                                                                                                                                                                                                           |             |               |\n",
    "| Comparator(s) or control(s)                         | Covariates include mean follow-up duration (months), gender (male, female), and age (years).                                                                                                                                                                                                                                                                                                                                                                                                                             |             |               |\n",
    "| Main outcomes                                       | International Knee Documentation Committee (IKDC) Subjective Knee Form, Lysholm Knee Scoring Scale, Foot and Ankle Disability Index (FADI), American Orthopaedic Foot and Ankle Society (AOFAS) Ankle Hindfoot Scale                                                                                                                                                                                                                                                                                                     |             |               |\n",
    "| Additional outcomes                                 | Graft failure (clinical, rupture), other knee outcome scores and ankle donor-site morbidity                                                                                                                                                                                                                                                                                                                                                                                                                              |             |               |\n",
    "| Data extraction (selection and coding)              | A standardized data collection form will be created, piloted, and used for data extraction by two independent reviewers and inter-rater reliability will be calculated using Cohen's κ and ICC co-efficients.                                                                                                                                                                                                                                                                                                            |             |               |\n",
    "| Risk of bias (quality) assessment                   | Randomized clinical trials (RCTs) will be assessed using Cochrane's revised RoB 2 tool and observational studies (prospective and retrospective cohort studies, case-control studies, and case series will be assessed using the MINORS criteria.                                                                                                                                                                                                                                                                        |             |               |\n",
    "| Strategy for data synthesis                         | Extracted outcome data will be transformed into standardized continuous (i.e., sample size, mean, standard deviation) and dichotomous (i.e., sample size, event) data and synthesized using meta-analysis of random effects into pooled means and prevalences. Within study means will undergo inverse-variance weighted and DerSimonian-Laird methods and proportions will undergo inverse logit transformation and restricted maximum likelihood (REML) methods for pooled estimate and standard errors, respectively. |             |               |\n",
    "| Analysis of subgroups or subsets                    | Subgroup analysis between PLT and PLBT will be performed to compare the effects of distal attachment during graft harvesting. Meta-regression analysis will be performed to control for the covariates (follow-up duration, mean age of participants, and gender) to determine whether the results are better explained by other predictors than variation in graft harvesting technique.                                                                                                                                |             |               |\n",
    "| Review team members                                 | Dong Woon Kim, MD Shayden Bernas Konrad Malinowski, MD, PhD                                                                                                                                                                                                                                                                                                                                                                                                                                                              |             |               |\n",
    "| Review affiliation                                  | Jagiellonian University Medical College, Kraków, Poland Artromedical Orthopedic Clinic, Bełchatów, Poland                                                                                                                                                                                                                                                                                                                                                                                                                |             |               |\n",
    "| Funding source                                      | N/A                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |             |               |\n",
    "| Named contact                                       | Dong Woon Kim (d.kim@student.uj.edu.pl)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |             |               |\n",
    "| Review timeline                                     | Started: 2025-10-12                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |             |               |\n",
    "| Date of first submission to PROSPERO                | 2025-10-16                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |             |               |\n",
    "| Date of registration in PROSPERO                    |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |             |               |\n",
    "| Publication of review results                       | Review results will be published for public access after acceptance for publication.                                                                                                                                                                                                                                                                                                                                                                                                                                     |             |               |\n",
    "| Stage of the review at this submission              | **Review stage**                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         | **Started** | **Completed** |\n",
    "| Screening search results against inclusion criteria | 17.10.2025                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |             |               |\n",
    "| Data synthesis                                      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |             |               |\n",
    "| Pilot work                                          |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |             |               |\n",
    "| Data extraction or receipt of IP                    |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |             |               |\n",
    "| Formal searching/study identification               |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |             |               |\n",
    "| Risk of bias/quality assessment                     |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |             |               |\n",
    "\n",
    "Review status\n",
    "\n",
    "Review conflict of interest\n",
    "\n",
    "None.\n",
    "\n",
    "Country\n",
    "\n",
    "Poland\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeceb371-df67-4263-b4e1-72173030bade",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6aec43b-3b40-4cac-949d-4227b08e76e8",
   "metadata": {},
   "source": [
    "## Search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d0f0c97-e91c-4ad8-bf3c-56cdc3a72f8c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9536b2fa-99cf-48eb-ad7f-806eca9d08a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c8199b27-891b-4e8e-b0a9-cbc820ffcc76",
   "metadata": {},
   "source": [
    "## Deduplication\n",
    "\n",
    "Documentation of python scripts used for deduplication of search results (records).\n",
    "\n",
    "2025-10-22: Created custom python script (path: '../src/**deduplication.py**') to deduplicate based on selected column(s).   \n",
    "2025-10-22: Performed using doi, study_id (last name of first author + year of publication), and short_title (first 25 characters, trimmed and case-insensitized title).  \n",
    "2025-10-23: TODO - standardize journal names, and create loop-function for standalone deduplication based on multiple columns in a step-wise fashion. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7c1499f1-b0c7-4018-a7a4-4a726023bb82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('data/records.csv')\n",
    "df['study'] = df['first_author'] + ' ' + df['year'].astype(str)\n",
    "df.to_csv('data/records-revised.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3abbd052-922d-4985-84fb-e4ec7a7449cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter file name:  records\n",
      "Enter the columns for which to deduplicate based on:  doi\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Path.replace() takes 2 positional arguments but 3 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 50\u001b[39m\n\u001b[32m     47\u001b[39m     \u001b[38;5;28mprint\u001b[39m(results)\n\u001b[32m     49\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m50\u001b[39m     results, df_deduplicated, df_kept, df_removed, output_file_name, prisma_file_name = \u001b[43mdeduplicate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcols\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     52\u001b[39m     graph_text = \u001b[33mf\u001b[39m\u001b[33m\"\"\"\u001b[39m\u001b[33m---\u001b[39m\n\u001b[32m     53\u001b[39m \u001b[33mconfig:\u001b[39m\n\u001b[32m     54\u001b[39m \u001b[33m  theme: neutral\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m     71\u001b[39m \u001b[33mD & F --> H;\u001b[39m\n\u001b[32m     72\u001b[39m \u001b[33mB & H --> I\u001b[39m\u001b[33m\"\"\"\u001b[39m\n\u001b[32m     75\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(prisma_file_name, \u001b[33m\"\u001b[39m\u001b[33mw\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 43\u001b[39m, in \u001b[36mdeduplicate\u001b[39m\u001b[34m(df, cols)\u001b[39m\n\u001b[32m     30\u001b[39m results = {\n\u001b[32m     31\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mrecords\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mlen\u001b[39m(df),\n\u001b[32m     32\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mnulls\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mlen\u001b[39m(df_nulls),\n\u001b[32m   (...)\u001b[39m\u001b[32m     39\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mdeduplicated\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mlen\u001b[39m(df_deduplicated)\n\u001b[32m     40\u001b[39m }\n\u001b[32m     42\u001b[39m df_deduplicated.to_csv(output_file_name, index = \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m---> \u001b[39m\u001b[32m43\u001b[39m df_removed.to_csv(\u001b[43moutput_file_name\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreplace\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m.csv\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m_removed.csv\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m, index = \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m     46\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m results, df_deduplicated, df_kept, df_removed, output_file_name, prisma_file_name\n\u001b[32m     47\u001b[39m \u001b[38;5;28mprint\u001b[39m(results)\n",
      "\u001b[31mTypeError\u001b[39m: Path.replace() takes 2 positional arguments but 3 were given"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import mermaid as md\n",
    "from mermaid.graph import Graph\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "\n",
    "def deduplicate(df, cols):\n",
    "    input_file_name = 'data/search/' + input('Enter file name: ') + '.csv'\n",
    "    df = pd.read_csv(input_file_name) # A (records)\n",
    "    cols_input = input('Enter the columns for which to deduplicate based on: ')\n",
    "    cols = [c.strip() for c in cols_input.split(',')]\n",
    "    output_dir = Path(r'data/deduplication') / '_'.join(cols)\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    output_file_name = output_dir / f'records.csv'\n",
    "    prisma_file_name = output_file_name.with_suffix('.mmd')\n",
    "\n",
    "    nulls_mask = df[cols].isnull().any(axis=1)\n",
    "    df_nulls = df[nulls_mask] # B\n",
    "    df_non_nulls = df[~nulls_mask] # C\n",
    "    \n",
    "    duplicates_mask = df_non_nulls.duplicated(subset = cols, keep = False)\n",
    "    df_non_duplicates = df_non_nulls[~duplicates_mask] # D\n",
    "    df_duplicates = df_non_nulls[duplicates_mask] # E\n",
    "    df_kept = df_duplicates.drop_duplicates(subset = cols, keep = 'first')\n",
    "    df_removed = df_duplicates[~df_duplicates.index.isin(df_kept.index)]\n",
    "    df_unique = df_non_nulls.drop_duplicates(subset = cols, keep = 'first') # df of unique\n",
    "    df_deduplicated = pd.concat([df_unique, df_nulls], ignore_index=True) # df of unique + df of nulls\n",
    "\n",
    "    results = {\n",
    "        \"records\": len(df),\n",
    "        \"nulls\": len(df_nulls),\n",
    "        \"non_nulls\": len(df_non_nulls),\n",
    "        \"non_duplicates\": len(df_non_duplicates),\n",
    "        \"duplicates\": len(df_duplicates),\n",
    "        \"removed\": len(df_removed),\n",
    "        \"kept\": len(df_kept),\n",
    "        \"unique\": len(df_unique),\n",
    "        \"deduplicated\": len(df_deduplicated)\n",
    "    }\n",
    "    \n",
    "    df_deduplicated.to_csv(output_file_name, index = False)\n",
    "    df_removed.to_csv(output_file_name.replace('.csv', '_removed.csv'), index = False)\n",
    "\n",
    "    \n",
    "    return results, df_deduplicated, df_kept, df_removed, output_file_name, prisma_file_name\n",
    "    print(results)\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    results, df_deduplicated, df_kept, df_removed, output_file_name, prisma_file_name = deduplicate(df=None, cols=None)\n",
    "    \n",
    "    graph_text = f\"\"\"---\n",
    "config:\n",
    "  theme: neutral\n",
    "  curve: stepBefore\n",
    "---\n",
    "graph TD;\n",
    "A[\"**records** (*n* = {results['records']})\"];\n",
    "B[\"null (*n* = {results['nulls']})\"];\n",
    "C[\"non-null (*n* = {results['non_nulls']})\"];\n",
    "D[\"non-duplicates (*n* = {results['non_duplicates']})\"];\n",
    "E[\"duplicates (*n* = {results['duplicates']})\"];\n",
    "F[\"duplicates kept (*n* = {results['kept']})\"];\n",
    "G[\"duplicates removed (*n* = {results['removed']})\"];\n",
    "H[\"unique (*n* = {results['unique']})\"];\n",
    "I[\"deduplicated (*n* = {results['deduplicated']})\"];\n",
    "\n",
    "A --> B & C;\n",
    "C --> D & E;\n",
    "E --> F & G;\n",
    "D & F --> H;\n",
    "B & H --> I\"\"\"\n",
    "    \n",
    "\n",
    "    with open(prisma_file_name, \"w\") as f:\n",
    "        f.write(graph_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "303ee809-b26a-4fb9-a3da-8f4e51aef0d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter file name:  deduplication/doi/records\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/search/deduplication/doi/records.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 50\u001b[39m\n\u001b[32m     47\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m results, df_deduplicated, df_kept, df_removed, output_file_name\n\u001b[32m     49\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m50\u001b[39m     \u001b[43mdeduplicate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 10\u001b[39m, in \u001b[36mdeduplicate\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdeduplicate\u001b[39m():\n\u001b[32m      9\u001b[39m     input_file_name = \u001b[33m'\u001b[39m\u001b[33mdata/search/\u001b[39m\u001b[33m'\u001b[39m + \u001b[38;5;28minput\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mEnter file name: \u001b[39m\u001b[33m'\u001b[39m) + \u001b[33m'\u001b[39m\u001b[33m.csv\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m     df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_file_name\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# A (records)\u001b[39;00m\n\u001b[32m     11\u001b[39m     cols_input = \u001b[38;5;28minput\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mEnter the columns for which to deduplicate based on: \u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     12\u001b[39m     cols = [c.strip() \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m cols_input.split(\u001b[33m'\u001b[39m\u001b[33m,\u001b[39m\u001b[33m'\u001b[39m)]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Python313\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Python313\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    617\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    619\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Python313\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1617\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1619\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1620\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Python313\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1878\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[32m   1879\u001b[39m         mode += \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1880\u001b[39m \u001b[38;5;28mself\u001b[39m.handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1881\u001b[39m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1882\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1883\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1884\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcompression\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1885\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmemory_map\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1886\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1887\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding_errors\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstrict\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1888\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstorage_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1889\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1890\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1891\u001b[39m f = \u001b[38;5;28mself\u001b[39m.handles.handle\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Python313\\Lib\\site-packages\\pandas\\io\\common.py:873\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    868\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    869\u001b[39m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[32m    870\u001b[39m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[32m    871\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ioargs.encoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs.mode:\n\u001b[32m    872\u001b[39m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m873\u001b[39m         handle = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    874\u001b[39m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    875\u001b[39m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    876\u001b[39m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    877\u001b[39m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    878\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    880\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    881\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m    882\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(handle, ioargs.mode)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'data/search/deduplication/doi/records.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import mermaid as md\n",
    "from mermaid.graph import Graph\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "\n",
    "def deduplicate():\n",
    "    input_file_name = 'data/search/' + input('Enter file name: ') + '.csv'\n",
    "    df = pd.read_csv(input_file_name) # A (records)\n",
    "    cols_input = input('Enter the columns for which to deduplicate based on: ')\n",
    "    cols = [c.strip() for c in cols_input.split(',')]\n",
    "    output_dir = Path(r'data/deduplication') / '_'.join(cols)\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    output_file_name = output_dir / f'records.csv'\n",
    "    \n",
    "    nulls_mask = df[cols].isnull().any(axis=1)\n",
    "    df_nulls = df[nulls_mask] # B\n",
    "    df_non_nulls = df[~nulls_mask] # C\n",
    "    \n",
    "    duplicates_mask = df_non_nulls.duplicated(subset = cols, keep = False)\n",
    "    df_non_duplicates = df_non_nulls[~duplicates_mask] # D\n",
    "    df_duplicates = df_non_nulls[duplicates_mask] # E\n",
    "    df_kept = df_duplicates.drop_duplicates(subset = cols, keep = 'first')\n",
    "    df_removed = df_duplicates[~df_duplicates.index.isin(df_kept.index)]\n",
    "    df_unique = df_non_nulls.drop_duplicates(subset = cols, keep = 'first') # df of unique\n",
    "    df_deduplicated = pd.concat([df_unique, df_nulls], ignore_index=True) # df of unique + df of nulls\n",
    "\n",
    "    results = {\n",
    "        \"records\": len(df),\n",
    "        \"nulls\": len(df_nulls),\n",
    "        \"non_nulls\": len(df_non_nulls),\n",
    "        \"non_duplicates\": len(df_non_duplicates),\n",
    "        \"duplicates\": len(df_duplicates),\n",
    "        \"removed\": len(df_removed),\n",
    "        \"kept\": len(df_kept),\n",
    "        \"unique\": len(df_unique),\n",
    "        \"deduplicated\": len(df_deduplicated)\n",
    "    }\n",
    "\n",
    "    print(results)\n",
    "    \n",
    "    df_deduplicated.to_csv(output_file_name, index = False)\n",
    "    df_removed.to_csv(output_file_name.with_name(output_file_name.stem + '_removed.csv'), index = False)\n",
    "\n",
    "    \n",
    "    return results, df_deduplicated, df_kept, df_removed, output_file_name\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    deduplicate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71411a6b-14df-498a-9995-cd06adbf13ae",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90319851-a537-477b-944b-57348fe2c9eb",
   "metadata": {},
   "source": [
    "## Screening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d66f298d-5445-4bf8-8471-3349e7095ba9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E:\\20-29 projects\\.github\\repositories\\srma\\data\\prisma_revised.csv\n",
      "E:\\20-29 projects\\.github\\repositories\\srma\\data\\.ipynb_checkpoints\\2025-10-12_pubmed-checkpoint.csv\n",
      "E:\\20-29 projects\\.github\\repositories\\srma\\data\\.ipynb_checkpoints\\records-checkpoint.csv\n",
      "E:\\20-29 projects\\.github\\repositories\\srma\\data\\.ipynb_checkpoints\\doi_deduplicated-checkpoint.csv\n",
      "E:\\20-29 projects\\.github\\repositories\\srma\\data\\.ipynb_checkpoints\\study_id_deduplicated-checkpoint.csv\n",
      "E:\\20-29 projects\\.github\\repositories\\srma\\data\\.ipynb_checkpoints\\title_deduplicated-checkpoint.csv\n",
      "E:\\20-29 projects\\.github\\repositories\\srma\\data\\.ipynb_checkpoints\\doi_deduplicated_removed-checkpoint.csv\n",
      "E:\\20-29 projects\\.github\\repositories\\srma\\data\\.ipynb_checkpoints\\records-revised-checkpoint.csv\n",
      "E:\\20-29 projects\\.github\\repositories\\srma\\data\\.ipynb_checkpoints\\doi_study_deduplicated-checkpoint.csv\n",
      "E:\\20-29 projects\\.github\\repositories\\srma\\data\\.ipynb_checkpoints\\study_deduplicated-checkpoint.csv\n",
      "E:\\20-29 projects\\.github\\repositories\\srma\\data\\search\\records.csv\n",
      "E:\\20-29 projects\\.github\\repositories\\srma\\data\\search\\exports\\2025-10-12_pubmed.csv\n",
      "E:\\20-29 projects\\.github\\repositories\\srma\\data\\search\\exports\\2025-10-15_embase.csv\n",
      "E:\\20-29 projects\\.github\\repositories\\srma\\data\\deduplication\\doi_deduplicated_removed.csv\n",
      "E:\\20-29 projects\\.github\\repositories\\srma\\data\\deduplication\\short_title_deduplicated.csv\n",
      "E:\\20-29 projects\\.github\\repositories\\srma\\data\\deduplication\\short_title_deduplicated_removed.csv\n",
      "E:\\20-29 projects\\.github\\repositories\\srma\\data\\deduplication\\doi_deduplicated.csv\n",
      "E:\\20-29 projects\\.github\\repositories\\srma\\data\\screening\\full_text\\full_text_screening.csv\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import subprocess\n",
    "\n",
    "\n",
    "root_dir = Path(r'E:\\20-29 projects\\.github\\repositories\\srma')\n",
    "data_dir = root_dir / 'data'\n",
    "\n",
    "ps1 = f'Get-ChildItem -Path \"{data_dir}\" -Recurse -File -Filter \"*.csv\" | Select-Object -ExpandProperty FullName'\n",
    "result = subprocess.run(\n",
    "    [\"powershell\", \"-ExecutionPolicy\", \"Bypass\", \"-Command\", ps1],\n",
    "    capture_output=True,  # Capture stdout/stderr\n",
    "    text=True             # Decode output as string\n",
    ")\n",
    "\n",
    "print(result.stdout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d6f7f4-627f-4ca1-935d-38628005952a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
